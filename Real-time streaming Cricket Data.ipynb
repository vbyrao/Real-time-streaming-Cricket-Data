{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "174516cc-5988-441e-9a53-8776fda118a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca563e39-31c3-42e9-b2df-de7b52965b96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-eventhub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "097e5e5f-ca64-413e-829b-fb81e4bd55db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from azure.eventhub import EventHubProducerClient, EventData\n",
    "\n",
    "# CricAPI credentials\n",
    "API_KEY = \"b213b9b0-2e50-4cae-90f5-15029cfa4d9b\"\n",
    "CRICAPI_URL = \"https://api.cricapi.com/v1/cricScore?apikey=\" + API_KEY  # API URL\n",
    "\n",
    "# Event Hub credentials\n",
    "EVENT_HUB_CONN_STR = \"Endpoint=sb://cricket-data-stream-ns.servicebus.windows.net/;SharedAccessKeyName=Data-send;SharedAccessKey=Y/QuiMSZqEbxoZPU/NLgdnky72tXDxZ1Q+AEhC93Mpw=;EntityPath=cricket-live-data\"\n",
    "EVENT_HUB_NAME = \"cricket-live-data\"\n",
    "\n",
    "# Function to fetch live cricket match data from CricAPI\n",
    "def fetch_cricket_data():\n",
    "    response = requests.get(CRICAPI_URL)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Failed to fetch data from CricAPI, status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to send data to Event Hubs\n",
    "def send_data_to_event_hub(data):\n",
    "    producer = EventHubProducerClient.from_connection_string(conn_str=EVENT_HUB_CONN_STR, eventhub_name=EVENT_HUB_NAME)\n",
    "    event_data_batch = producer.create_batch()\n",
    "    event_data_batch.add(EventData(json.dumps(data)))\n",
    "    producer.send_batch(event_data_batch)\n",
    "    producer.close()\n",
    "\n",
    "# Main function to fetch and send data\n",
    "if __name__ == \"__main__\":\n",
    "    cricket_data = fetch_cricket_data()\n",
    "    if cricket_data:\n",
    "        print(\"Fetched cricket data successfully\")\n",
    "        send_data_to_event_hub(cricket_data)\n",
    "        print(\"Data sent to Event Hub successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca65e2c1-c9f2-4602-87c5-5f8dd94cd2b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if Unity Catalog is enabled and you have permissions to create catalogs\n",
    "try:\n",
    "    # Create the catalog\n",
    "    spark.sql(\"CREATE CATALOG IF NOT EXISTS streaming;\")\n",
    "    print(\"Catalog 'streaming' created or already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating catalog: {e}\")\n",
    "\n",
    "# Create schemas (or databases) within the catalog\n",
    "try:\n",
    "    spark.sql(\"CREATE SCHEMA IF NOT EXISTS streaming.bronze;\")\n",
    "    print(\"Schema 'streaming.bronze' created or already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating bronze schema: {e}\")\n",
    "\n",
    "try:\n",
    "    spark.sql(\"CREATE SCHEMA IF NOT EXISTS streaming.silver;\")\n",
    "    print(\"Schema 'streaming.silver' created or already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating silver schema: {e}\")\n",
    "\n",
    "try:\n",
    "    spark.sql(\"CREATE SCHEMA IF NOT EXISTS streaming.gold;\")\n",
    "    print(\"Schema 'streaming.gold' created or already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating gold schema: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "431e14b8-6de9-4d98-b4a3-b39c5313fdfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List schemas in the default metastore (hive_metastore)\n",
    "spark.sql(\"SHOW SCHEMAS IN streaming\").show()\n",
    "\n",
    "# Verify the default catalog (should be hive_metastore)\n",
    "spark.sql(\"SHOW CATALOGS\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbd68286-5327-461e-985b-ea57c1eba913",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Assuming 'spark' is your SparkSession\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Define the Event Hub connection string\n",
    "event_hub_connection_string = \"Endpoint=sb://cricket-data-stream-ns.servicebus.windows.net/;SharedAccessKeyName=Data-send;SharedAccessKey=Y/QuiMSZqEbxoZPU/NLgdnky72tXDxZ1Q+AEhC93Mpw=;EntityPath=cricket-live-data\"\n",
    "\n",
    "# Encrypt the connection string using Spark's Scala API\n",
    "encrypted_connection_string = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(event_hub_connection_string)\n",
    "\n",
    "# Configure the Event Hub settings with the encrypted connection string\n",
    "ehConf = {\n",
    "  'eventhubs.connectionString': sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(event_hub_connection_string)\n",
    "}\n",
    "\n",
    "# Continue with your existing code to read from Event Hubs stream\n",
    "event_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"eventhubs\")\n",
    "    .options(**ehConf)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# The rest of your code remains the same\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define the schema for the incoming JSON data\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"dateTimeGMT\", StringType(), True),\n",
    "    StructField(\"matchType\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"ms\", StringType(), True),\n",
    "    StructField(\"t1\", StringType(), True),\n",
    "    StructField(\"t2\", StringType(), True),\n",
    "    StructField(\"t1s\", StringType(), True),\n",
    "    StructField(\"t2s\", StringType(), True),\n",
    "    StructField(\"series\", StringType(), True),\n",
    "    StructField(\"t1img\", StringType(), True),\n",
    "    StructField(\"t2img\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Event Hub connection string\n",
    "event_hub_connection_string = \"Endpoint=sb://cricket-data-stream-ns.servicebus.windows.net/;SharedAccessKeyName=Data-send;SharedAccessKey=Y/QuiMSZqEbxoZPU/NLgdnky72tXDxZ1Q+AEhC93Mpw=;EntityPath=cricket-live-data\"\n",
    "\n",
    "# Configure the Event Hub settings\n",
    "ehConf = {\n",
    "  'eventhubs.connectionString': event_hub_connection_string\n",
    "}\n",
    "\n",
    "# Read from Event Hubs stream\n",
    "event_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"eventhubs\")\n",
    "    .options(**ehConf)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Convert binary data in 'body' to string (assuming the body contains UTF-8 encoded JSON)\n",
    "cricket_data = event_stream.withColumn(\"body\", col(\"body\").cast(\"string\"))\n",
    "\n",
    "# Parse the body column (JSON data) and apply the schema\n",
    "cricket_data_json = cricket_data.select(from_json(col(\"body\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Write raw data to the Bronze layer (Delta table)\n",
    "cricket_data_json.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/mnt/checkpoints/cricket-data\") \\\n",
    "    .start(\"/mnt/bronze/cricket-data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53079656-f5c4-4651-ab98-f1788ce5ced0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Real-time streaming Cricket Data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
